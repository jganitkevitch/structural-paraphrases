\documentclass[11pt]{article}
\usepackage{acl-hlt2011}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{url}
\usepackage{multirow}
\usepackage{rotating}
%\setlength\titlebox{6.5cm}    
% You can expand the title box if you really have to


\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\mnote}[1]{\marginpar{%
  \vskip-\baselineskip
  \raggedright\footnotesize
  \itshape\hrule\smallskip\footnotesize{#1}\par\smallskip\hrule}}  

%\title{Text-to-Text Generation with Syntactic Paraphrases Learned from Bilingual Parallel Corpora}
\title{Learning Sentential Paraphrases from Bilingual Parallel Corpora \\ for Text-to-Text Generation}

\author{Juri Ganitkevitch \and Chris Callison-Burch\\ 
Center for Language and Speech Processing\\ 
Johns Hopkins University}


\author{X\\ 
x\\ 
x\\ 
x}


\date{}

\begin{document}
\maketitle

\begin{abstract}
Previous work has shown that high quality {\it phrasal} paraphrases can be extracted from  bilingual parallel corpora.  However, it is not clear whether bitexts are an appropriate resource for extracting more sophisticated {\it structural} or {\it sentential} paraphrases, which are more obviously learnable from monolingual parallel corpora.
%  The extraction of paraphrases from bilingual corpora has been shown
%  to outperform monolingual approaches on both precision and
 % coverage. However, the learned paraphrases lack structural
 % information beyond the lexical or part-of-speech level, which limits
 % their expressive power and their suitability for sentential
 % paraphrasing. 
We extend bilingual paraphrase extraction to syntactic paraphrases
and demonstrate its ability to learn variety of general paraphrastic transformations, 
including passivization, dative shift, topicalization, etc.  We discuss how our model can be adapted to many text generation tasks, by augmenting its feature set, development data and parameter estimation routine.  We illustrate this adaptation by using our bilingually extracted paraphrases for the task of sentence compression. 
%   \begin{itemize}
%   \item ``in this work'' stuff is implied
%   \item newcites rather than cites 
%   \item put a little brag claim at the end, summarizing the outcome
%   \item abstract should tell you whether paper is interesting, whether
%     your results are interesting
%   \item motivate the problem
%   \item briefly motivate choices
%   \item be clear, sell the paper to the reviewers
%   \item be direct, motivate and explain instead of saying ``we
%     motivate/explain''
%   \item briefly tie back to previous work
%   \item concrete numbers
%   \end{itemize}
\end{abstract}


\section{Introduction} \label{introduction}

Paraphrases are alternative ways of expressing the same information. 
Automatically generating and detecting paraphrases is a crucial aspect of many 
natural language processing tasks.
In multi-document summarization, paraphrase detection is used
to collapse redundancies \cite{Barzilay1999,BarzilayThesis}. Paraphrase generation can be used 
for query expansion in information retrieval and question
answering systems \cite{mckeown:1979:ACL,Anick1999,Ravichandran2002,Riezler2007}. 
Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization \cite{Zhou2006b,Kauchak2006,Owczarzak2006,Madnani2007,Snover2010}. 
%Paraphrases are used to generate additional reference translations for statistical machine translation \cite{Madnani2007} and for more flexible matching when evaluating MT output or automatically generated summaries  \cite{Zhou2006b,Kauchak2006,Owczarzak2006}.

Coarsely, we can distinguish two forms of paraphrases: \emph{phrasal
  paraphrases} denote a set of surface text forms with the same
meaning:
\begin{center}
\begin{tabular}{c}
the committee's second proposal \\
the second proposal of the committee ,
\end{tabular}
\end{center}
while \emph{syntactic paraphrases} augment the surface forms by
introducing nonterminals (or \emph{slots}) that are annotated with
syntactic constraints:
\begin{center}
\begin{tabular}{c}
the $\mathit{NP}_1$'s $\mathit{NP}_2$ \\
the $\mathit{NP}_2$ of the $\mathit{NP}_1$
\end{tabular}
\end{center}
It is evident that the latter have a much higher potential for
generalization and for capturing interesting paraphrastic transformations.

A variety of different types of corpora (and semantic equivalence
cues), have been used to automatically induce paraphrase collections
for English \cite{Madnani2010}. The perhaps most natural type of corpus for this task is
a monolingual parallel text, from which paraphrases can be extracted
by leveraging the knowledge that the given sentence pairs are perfect
paraphrases of each other \cite{Barzilay2001,Pang2003}. While rich
syntactic paraphrases have been learned from such corpora, they suffer
from very limited data availability and are thus limited to poor
coverage.

Other methods strive to obtain paraphrases from raw monolingual text,
replacing the exact correspondence of sentences in monolingual parallel corpora with
distributional similarity \cite{Lin2001,Bhagat2008}. While
vast amounts of data are readily available for these approaches, the
correspondency information they employ is weaker and suffers from
problems such as mistaking cousin expressions or antonyms (such as
$\{\mathit{boy}, \mathit{girl}\}$ or $\{\mathit{rise},
\mathit{fall}\}$) for paraphrases.

Abundantly available bilingual parallel corpora have been shown to
address both these issues, obtaining paraphrases via a pivoting step
over foreign language phrases \cite{Callison-Burch2005}. The coverage
of paraphrase lexica extracted from bitexts has been shown to
outperform that obtained from other sources \cite{Zhao2008b}. However,
despite existing work on the extraction of more powerful paraphrases
\cite{Madnani2007,Callison-Burch2008,cohn-lapata:2008,Zhao2008}, it is not clear the extent to which sentential paraphrasing can be induced from bitexts. In this paper
we:
\begin{itemize}
\item Extend the bilingual pivoting approach to paraphrase induction
  to produce rich syntactic paraphrases.
\item Perform a thorough analysis of the types of paraphrases we obtain discuss the
  paraphrastic transformations we are capable of capturing.
\item Show the resulting paraphrase grammars' fitness and adaptability
  for a variety of text-to-text generation tasks.
\item Describe how training paradigms for
  syntactic/sentential paraphrase models should be tailored to different text-to-text generation tasks. 
\end{itemize}


\section{Related Work} \label{related_work}

Paraphrase extraction using bilingual parallel corpora was proposed by \newcite{Callison-Burch2005}
who induced paraphrases using techniques from {\it phrase-based}
statistical machine translation \cite{Koehn2003}. After extracting a
bilingual phrase table, English paraphrases can be obtained by
pivoting through foreign language phrases. 
%The phrase table contains
%phrase pairs $(e, f)$ (where the $e$ and $f$ stand for English and
%foreign phrases, respectively) as well as bi-directional 
Since many
paraphrases can be extracted for a phrase,, \newcite{Callison-Burch2005} rank them using a paraphrase probability defined in terms of the translation model
probabilities $p(f | e)$ and $p(e | f)$: 

\begin{eqnarray}
  p(e_2|e_1) &=& \sum_f p(e_2,f|e_1)\\
                  &=& \sum_f p(e_2|f,e_1) p(f|e_1) \\
                  &\approx& \sum_f p(e_2|f) p(f|e_1)
\label{paraphrase_prob_eqn}
\end{eqnarray}

Several subsequent efforts extended the bilingual pivoting technique, many of which introduced 
elements of more contemporary syntax-based approaches to statistical machine translation.   
\newcite{Madnani2007} extend the technique to {\it hierarchical}
phrase-based machine translation \cite{Chiang2005}, which is formally a synchronous context free grammar (SCFG), but which only uses a single non-terminal symbol ``X'' instead of linguistically informed non-terminals.


Hierarchical
phrases contain a variable 
\newcite{Madnani2007}'s paraphrase table contains these slotted
patterns as well. Because hierarchical phrase-based machine
translation ,
\newcite{Madnani2007}'s paraphrase table can be thought of as a {\it
  paraphrase grammar}. Their paraphrase grammar can paraphrase (or
``decode'') input sentences using an SCFG decoder, like the Hiero MT
system. \newcite{Madnani2007} mirror Hiero's log-linear model and its
feature set. The parameters for the log-linear model are estimated
using minimum error rate training, maximizing the BLEU metric on a set
of parallel English sentences. The authors report significant gains in
translation quality when using additional references generated by
paraphrasing to tune a machine translation system.



\newcite{Callison-Burch2008} constrained 
paraphrases to be the same syntactic constituent, 


\newcite{Zhao2008} further enrich pivot-based paraphrase approach with
syntactic information by extracting partial subtrees from a
dependency-parsed English side of a bitext and pivoting over the
corresponding Chinese phrases to extract paraphrases. The slots in the
resulting patterns are labeled with part-of-speech tags (but not
larger syntactic constituents). Their system also employs a log-linear
model that combines translation and lexical probabilities and is tuned
to maximize precision over a hand-labeled set of paraphrases.

Several research efforts have leveraged parallel monolingual corpora,
however they jointly suffer from the scarcity and noisiness of
parallel corpora.  \newcite{Dolan2004} work around this issue by extracting
parallel sentences from the vast amount of freely available comparable
English text and apply machine translation techniques to create a
paraphrasing system \cite{Quirk2004}. However, the word-based
translation model and monotone decoder they use results in a
substantial amount of identity paraphrases or single-word
substitutions.

%Relying on small data sets of semantically equivalent translations, \newcite{Pang2003} created finite state automata by syntax-aligning parallel sentences, enabling the generation of additional reference translations.

%Both \newcite{Barzilay2001} and \newcite{Ibrahim2003} sentence-align existing noisy parallel monolingual corpora such as translations of the same novels. While \newcite{Ibrahim2003} employ a set of heuristics that rely on anchor words identified by textual identity or matchin liguistic features such as gender, number or semantic class, \newcite{Barzilay2001} use a co-training approach that leverages context similarity to identify viable paraphrases.

%Semantic parallelism is well-established as a stong basis for the extraction of correspondencies such as paraphrases. However, there are notable efforts that choose to forgo it in favor of clustering approaches based on distributional characteristics. The well-known DIRT method by \newcite{Lin2001} fully relies on distributional similarity features for paraphrase extraction. Patterns extracted from paths in dependency graphs are clustered based on the similarity of the observed contents of their slots.

%Similarly, \newcite{Bhagat2008} argue that vast amounts of text can be leveraged to make up for the relative weakness of distributional features compared to parallelism. They also forgo complex annotations such as syntactic or dependency parses, relying only on part-of-speech tags to inform their approach. In their work, relations are learned by finding pattern clusters initially seeded by already known patterns. However, this method is not capable of producing syntactic paraphrases. \mnote{Need better tie-in with the overall theme of  structural paraphrases.}


\section{SCFGs in Translation} \label{formalism}

Similar to \newcite{Madnani2007}, our paraphrasing system borrows from
established parsing-based statistical machine translation
machinery. The model we use in our approach is a syntactically
informed \emph{ synchronous context-free grammar} (SCFG) that in
contrast to the Hiero formalism used by \newcite{Madnani2007} allows
for an arbitrary number of nonterminals.

Formally, a PSCFG $\mathcal{G}$ is defined by specifying
\[
\mathcal{G} = \langle \mathcal{N}, \mathcal{T}_S, \mathcal{T}_T,
\mathcal{R}, S \rangle ,
\]
where $\mathcal{N}$ is a set of nonterminal symbols, $\mathcal{T}_S$
and $\mathcal{T}_T$ are the source and target language vocabularies,
$\mathcal{R}$ is a set of rules and $S \in \mathcal{N}$ is the root
symbol. The rules in $\mathcal{R}$ take the form
\begin{equation*}
  C \rightarrow \langle \gamma, \alpha, \sim, w \rangle ,
\end{equation*}
where the rule's left-hand side $C \in \mathcal{N}$ is a nonterminal,
$\gamma \in (\mathcal{N} \cup \mathcal{T}_S)^*$ and $\alpha \in
(\mathcal{N} \cup \mathcal{T}_T)^*$ are strings of terminal and
nonterminal symbols with an equal number of nonterminals
$c_{\mathit{NT}}(\gamma) = c_{\mathit{NT}}(\alpha)$ and 
$$
\sim : \{1 \ldots c_{\mathit{NT}}(\gamma)\} \rightarrow \{1 \ldots
c_{\mathit{NT}}(\alpha)\}
$$ 
constitutes a one-to-one correspondency function between the
nonterminals in $\gamma$ and $\alpha$. A non-negative weight $w \geq
0$ is assigned to each rule, reflecting the likelihood of the rule.

\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.99\linewidth]{figures/scfg_grid_revamp.pdf}
\end{center}
\caption{TODO: adjust colors, check example.}
\label{samt_extraction}
\end{figure*}

The rule set for an SAMT grammar is extracted from word-aligned
sentence-parallel corpora where the target language side is annotated
with syntactic parses. Figure~\ref{samt_extraction} shows an example
of rules obtained for one phrase pair. To extract a rule, we first
choose a target span $t$. The left-hand side of the rule is then given
by the syntactic constituent governing $t$. In addition to single
constituent nonterminals, SAMT allows for the concatenation of
constituents as well as for CCG-style nonterminal labels
\cite{Steedman1999}, which increases the grammar's coverage by
allowing for non-syntactic phrases in the grammar. The source side of
the rule, $s$, is obtained by projecting $t$ over the word
alignment. To introduce nonterminals into the rule's source and
target sides, we can apply rules extracted over sub-phrases of $t$,
synchronously substituting the corresponding nonterminal symbol for
the sub-phrases on both sides. \mnote{Tie this back to $\alpha$ and
  $\gamma$?} The synchronous substitution applied to $t$ and $s$
yields the correspondency $\sim$.

Rather than assigning a weight $w$, SAMT stores a number of feature
values $\varphi_i$ for each rule, which are combined in a log-linear
model:
\begin{equation}
  w = \sum_i^N \lambda_i \varphi_i .
\end{equation}
For ease of notation, we will refer to $\varphi_1, \ldots ,\varphi_N$
as $\vec{\varphi}$. The feature set of the SAMT model by default
consists of 23 feature functions and covers a diverse set of rule
characteristics. We discuss our task-specific extensions to the SAMT
feature set in Section~\ref{adaptation}.

\section{SCFGs in Paraphrasing} \label{acquisition}

\begin{figure*}[!t]
\begin{center}
\includegraphics[width=0.99\linewidth]{figures/example_compression.pdf}
\end{center}
\caption{An example of a synchronous paraphrastic derivation. A few of
  the rules applied in the parse are show in the second column, with
  the pivot phrases that gave rise to them in the third.}
\end{figure*}

To create a paraphrase grammar from a translation grammar, we extend
the pivot approach of \newcite{Callison-Burch2005} to the SAMT
model. For this purpose, we assume a grammar that translates from a
given foreign language to English. For each pair of translation rules where
the left-hand side $C$ and foreign string $\gamma$ match
\begin{eqnarray*}
C \rightarrow \langle \gamma, \alpha_1, \sim_1, \vec{\varphi}_1 \rangle \\
C \rightarrow \langle \gamma, \alpha_2, \sim_2, \vec{\varphi}_2 \rangle
\end{eqnarray*}
we create a paraphrase rule
\begin{equation*}
C \rightarrow \langle \alpha_1, \alpha_2, \sim, \vec{\varphi} \rangle ,
\end{equation*}
where the nonterminal correspondency relation $\sim$ has been set to
reflect the combined nonterminal alignment
\begin{equation*}
\sim ~ = ~ \sim_1^{-1} \circ \sim_2 .
\end{equation*}

\subsection{Feature Computation}

The computation of $\vec{\varphi}$ from $\vec{\varphi}_1$ and
$\vec{\varphi}_2$ occurs on feature level. For this purpose, the SAMT
feature set can be roughly partitioned into three groups.

The first group consists of features that depend solely on the rule
itself. This includes features based on number or nature of terminals
or nonterminals in $\alpha_1$ or $\alpha_2$, features that indicate
nonterminal reordering or deletion of punctuation and pseudo-features
like a glue rule indicator or the application count feature. Since
none of these features require additional information, their
computation for $\vec{\varphi}$ is straightforward.

The features in the second group additionally depend on aggregate
counts over the bitext and thus cannot be computed from the rule
alone. However, the information contained in $\vec{\varphi}_1$ and
$\vec{\varphi}_2$ allows us to apply the pivot approach as shown in
Equation~\ref{paraphrase_prob_eqn} to approxate the real feature value
for $\vec{\varphi}$. In the SAMT feature set, this group only includes
the four lexical translation probability features.

The remainder of the feature set falls into the third group. These
features depend on information that we cannot reconstruct from
$\vec{\varphi}_1$ and $\vec{\varphi}_2$. Features in this group
include the rule's probability conditioned on the left-hand-side, its
source side with and without nonterminal labels as well as rareness
and unbalancedness penalties. For these we fall back to a black-box
approach, treating the value for a given features as an indication of
the rule's quality in some particular aspect. To combine the feature
values, we multiply for the relative frequencies and add for the
penalty functions.

In addition to combining $\vec{\varphi}_1$ and $\vec{\varphi}_2$, we
extend $\vec{\varphi}$ by a paraphrasing-specific boolean feature
$\varphi_{\mathit{ident}}$, that is $1$ when $\alpha_1 = \alpha_2$ and
$0$ otherwise. This allows us to tune our system to prefer or
dis-prefer identity paraphrases.


\subsection{Grammar Pruning}
\label{pruning}

Due to the diverse set of nonterminals allowed in our model, the
grammars we extract tend to become extremely large. This, combined
with the multiplicative effect of pivoting, quickly makes the
resulting paraphrase grammars grow too large too handle.

To keep the grammars at a manageable size while retaining good
paraphrases, we implement a the following pruning approach. We only
consider translation rules that have been seen more than a given
number of times and exceed a given translation probability threshold
for pivot recombination. In addition to this, we rank the generated
paraphrases for each pattern according to a combination of translation
and lexical probablity and only retain the top $n$
alternatives. \mnote{This is a little wonky, maybe drop the top-$n$
  pruning?}

The result of this process is a paraphrase grammar with syntactic
annotations and a feature set which (approximately) mirrors that of
the initial translation grammar. We now move to estimating the
feature weights. \mnote{This is awful. Needs better intro into
  estimation part.}


\section{Experimental Setup} \label{setup}

\begin{itemize}
\item The following examples and discussions are based on a paraphrase
  grammar we extraced from the French-English bitext that is part of
  EuroParl version 5.
\item We used the Berkely aligner and The Parser
\item SAMT pipeline
\end{itemize}


\section{Analysis} \label{analysis}

\begin{table*}[!ht]
  \begin{center}
  \begin{tabular}{|c|rrcl|}
    \hline
    \multirow{2}{*}{Possessive rule} & $\mathit{NP}$ $\rightarrow$ & the
    $\mathit{NN}$ of the $\mathit{NNP}$ & $\mid$ & the
    $\mathit{NNP}$'s $\mathit{NN}$ \\
    & $\mathit{NP}$ $\rightarrow$  & the $\mathit{NNS}_1$ made by
    $\mathit{NNS}_2$ & $\mid$ & the $\mathit{NNS}_2$'s
    $\mathit{NNS}_1$ \\
    \hline
    \multirow{2}{*}{Dative shift} & $\mathit{VP}$ $\rightarrow$ & give
    $\mathit{NN}$ to $\mathit{NP}$ & $\mid$ & give $\mathit{NP}$ the
    $\mathit{NN}$ \\
    & $\mathit{VP}$ $\rightarrow$ & provide $\mathit{NP}_1$ to
    $\mathit{NP}_2$ & $\mid$ & give $\mathit{NP}_2$
    $\mathit{NP}_1$ \\
    \hline
    \hline
    Adverbial phrase move & 
    $\mathit{S/VP}$ $\rightarrow$ & we $\mathit{VBP}$ $\mathit{ADVP}$
    & $\mid$ & $\mathit{ADVP}$, we $\mathit{VBP}$ \\
    \hline
    Reduced relative clause & $\mathit{SBAR/S}$ $\rightarrow$ &
    although $\mathit{PRP}$ $\mathit{VBP}$ that & $\mid$ &although $\mathit{PRP}$ $\mathit{VBP}$ \\
    \hline
    Topicalization  & $\mathit{S}$ $\rightarrow$ & $\mathit{NP}$,
    $\mathit{VP}$ . & $\mid$ & $\mathit{VP}$, $\mathit{NP}$ . \\
    \hline
    \hline
    Passivization &
    $\mathit{SBAR}$ $\rightarrow$ & that $\mathit{NP}$ had
    $\mathit{VBN}$ & $\mid$ & which was $\mathit{VBN}$ by $\mathit{NP}$ \\
    \hline
    Light verbs & $\mathit{VP}$ $\rightarrow$ & take action $\mathit{ADVP}$ &
    $\mid$ & to act $\mathit{ADVP}$ \\
    \hline
\end{tabular}
\end{center}
\caption{Examples of meaning-preserving transformations and syntactic
  paraphrases that our system extracts to capture them.}
\label{example_rules}
\end{table*}


A key motivation for the use of syntactic paraphrases over their
phrasal counterparts is their potential to capture meaning-preserving
linguistic transformations in a general fashion. In many cases a
phrasal system will be limited to memorizing the fully lexicalized
variants of such a transfomation in its paraphrase table, resulting in
poor generalization capabilities. A syntactic paraphrasing system
should be able to address this issue by learning syntactically
well-formed and generic patterns that can be easily applied to unseen
data.

To put this expectation to the test, we investigate how our grammar
captures a number of well-known paraphrastic
transformations. Table~\ref{example_rules} shows the transformations
along with examples of the generic grammar rules our system learns to
represent them. When given a transformation to extract a syntactic
paraphrase for, we want to find rules that neither under- nor
over-generalize. This means that, while replacing the maximum number
of \emph{syntactic} arguments with nonterminals, the rules ideally
will both retain enough lexicalization to serve as sufficient evidence
for the applicability of the transformation and impose constraints on
the nonterminals to ensure the arguments' well-formedness.

The paraphrases implementing the \emph{possessive rule} and the
\emph{dative shift} shown in Table~\ref{example_rules} are a good
examples of this: the two noun-phrase arguments to the expressions are
abstracted to nonterminals while the rules' lexicalization provides an
appropriate frame of evidence for the transform. This is important for
a good representation of the dative shift, which is a reordering
transformation that fully applies to certain di-transitive verbs while
other verbs are uncommon in one of the forms:
\begin{center}
\begin{tabular}{l}
  give \emph{decontamination equipment} to \emph{Japan} \\
  give \emph{Japan} \emph{decontamination equipment} \\
  \vspace{-10pt}\\
  provide \emph{decontamination equipment} to \emph{Japan} \\
  ? provide \emph{Japan} \emph{decontamination equipment} \\
\end{tabular}
\end{center}
Note how our system extracts a dative shift rule for \emph{to give}
and a rule that both shifts and substitutes a more appropriate verb
for the new form for \emph{to provide}.

\newcite{Madnani2007} generalize from phrasal paraphrases with the
Hiero formalism, which uses SCFG rules with a single nonterminal
$X$. We use syntactic nonterminals in our paraphrase rules to capture
complex transforms that also impose appropriate constraints on their
application. Since \newcite{Madnani2007} do not impose any constraints
on how the nonterminal $X$ can be realized, their equivalent of the
\emph{topicalization} rule
\mnote{I think that our version of the topicalization rule should be $\mathit{S}$ $\rightarrow$  $\mathit{S/NP}$ $\mathit{NP}$.  $\mid$ $\mathit{NP}$, $\mathit{S/NP}$.  I.e. \emph{I like these bagels!  $\mid$ These bagels, I like!}   }
\begin{center}
\begin{tabular}{rrcl}
  $\mathit{S}$ $\rightarrow$ & $\mathit{X}_1$,
  $\mathit{X}_2$ . & $\mid$ & $\mathit{X}_2$, $\mathit{X}_1$ . \\
\end{tabular}
\end{center}
would massively overgeneralize. Additional examples of transforms our
use of syntax allows us to capture are the \emph{adverbial phrase 
 shift} and the \emph{reduction of a relative clause}.
 
Unsurprisingly, syntactic information alone is not sufficient to
capture all transformations. For instance it is hard to extract
generic paraphrases for all instances of \emph{passivization}, since our syntactic
model currently has no means of representing the morphological changes that the
verb undergoes:
\begin{center}
\begin{tabular}{l}
%  the cat \emph{ate} the mouse \\
%  the mouse \emph{was eaten} by the cat \\
  the reactor \emph{leaks} radiation \\
  radiation \emph{is leaking} from the reactor\\
\end{tabular}

\end{center}
Still, for cases where verb's morphology does not change, we manage to
learn a rule:
\begin{center}
\begin{tabular}{l}
%  the mouse that the cat had \emph{eaten} \\
%  the mouse which was \emph{eaten} by the cat \\
the radiation that the reactor had \emph{leaked} \\
the radiation which \emph{leaked} from the reactor \\
\end{tabular}
\end{center}
%
Another example of a deficiency in our synchronous grammar models are
\emph{light verb} constructs such as
\begin{center}
\begin{tabular}{l}
  to take a \emph{walk} \\
  to \emph{walk} .
\end{tabular}
\end{center}
Here, a noun is transformed into the corresponding verb -- something
our synchronous syntactic CFG approach is not able to capture except
through memorization.

Overall our survey shows that we are able to extract appropriately generic
representations for a surprising number of paraphrastic
transformations, far exceeding the expressiveness of previous
approaches to paraphrase extraction from bilingual parallel corpora.



% \subsection{Comparison of Paraphrase
%   Patterns} \label{pattern_comparison}

% To analyze the paraphrase patterns produced by our system, we reduce
% the grammar to only patterns that apply to an example sentence and
% take a closer look at the most likely pattern pairs. We contrast our
% extracted paraphrases with a Hiero-style baseline that mirrors the
% approach of \newcite{Madnani2007}. 

% \begin{table*}[t]
% \begin{center}
% \begin{tabular}{|c|c|rrcl|}
%   \hline
%   Input phrase & \multicolumn{5}{c|}{Best extracted paraphrases} \\
%   \hline
%   \multirow{6}{*}{the dog's tail} &
%   \multirow{3}{*}{H}
%   &
%   $\mathit{X}$ $\rightarrow$  & the $\mathit{X}_1$'s
%   $\mathit{X}_2$ & $\mid$ & the $\mathit{X}_2$ of the $\mathit{X}_1$
%   \\
%   &&
%   $\mathit{X}$ $\rightarrow$  & the $\mathit{X}_1$'s
%   $\mathit{X}_2$ & $\mid$ & the $\mathit{X}_2$ of the $\mathit{X}_1$
%   \\
%   &&
%   $\mathit{X}$ $\rightarrow$  & the $\mathit{X}_1$'s
%   $\mathit{X}_2$ & $\mid$ & the $\mathit{X}_2$ of the $\mathit{X}_1$ \\
%   \cline{2-6}
%   & \multirow{3}{*}{S}
%   &
%   $\mathit{NP}$ $\rightarrow$  & the $\mathit{NNP}_1$'s
%   $\mathit{NN}_2$ & $\mid$ & the $\mathit{NN}_2$ of the $\mathit{NNP}_1$
%   \\
%   &&
%   $\mathit{NP}$ $\rightarrow$  & the $\mathit{NNP}_1$'s
%   $\mathit{NN}_2$ & $\mid$ & the $\mathit{NN}_2$ of the $\mathit{NNP}_1$
%   \\
%   &&
%   $\mathit{NP}$ $\rightarrow$  & the $\mathit{NNP}_1$'s
%   $\mathit{NN}_2$ & $\mid$ & the $\mathit{NN}_2$ of the
%   $\mathit{NNP}_1$ \\
%   \hline
%   \multirow{6}{*}{the dog's tail} &
%   \multirow{3}{*}{H}
%   &
%   $\mathit{X}$ $\rightarrow$  & the $\mathit{X}_1$'s
%   $\mathit{X}_2$ & $\mid$ & the $\mathit{X}_2$ of the $\mathit{X}_1$
%   \\
%   &&
%   $\mathit{X}$ $\rightarrow$  & the $\mathit{X}_1$'s
%   $\mathit{X}_2$ & $\mid$ & the $\mathit{X}_2$ of the $\mathit{X}_1$  \\
%   &&
%   $\mathit{X}$ $\rightarrow$  & the $\mathit{X}_1$'s
%   $\mathit{X}_2$ & $\mid$ & the $\mathit{X}_2$ of the $\mathit{X}_1$
%   \\
%   \cline{2-6}
%   & \multirow{3}{*}{S}
%   &
%   $\mathit{NP}$ $\rightarrow$  & the $\mathit{NNP}_1$'s
%   $\mathit{NN}_2$ & $\mid$ & the $\mathit{NN}_2$ of the $\mathit{NNP}_1$
%   \\
%   &&
%   $\mathit{NP}$ $\rightarrow$  & the $\mathit{NNP}_1$'s
%   $\mathit{NN}_2$ & $\mid$ & the $\mathit{NN}_2$ of the $\mathit{NNP}_1$
%   \\
%   &&
%   $\mathit{NP}$ $\rightarrow$  & the $\mathit{NNP}_1$'s
%   $\mathit{NN}_2$ & $\mid$ & the $\mathit{NN}_2$ of the
%   $\mathit{NNP}_1$ \\
%   \hline
% \end{tabular}
% \end{center}
% \caption{This table shows the top paraphrase rules matching an input
%   phrase as ranked by our (S) and the baseline system (H).}
% \end{table*}


\section{Text-to-Text Applications} \label{adaptation}

The core of many text-to-text generation tasks is sentential
paraphrasing, augmented with specific constraints or
goals. Since in our model we are borrowing much of the machinery 
from statistical machine translation -- a sentential rewriting problem
itself -- it is straightforward to use our paraphrase grammars to generate new sentences using its decoding and parameter optimization techniques. The SMT framework  can be adapted to 
many different text-to-text generation tasks. 
These could include text simplification, sentence compression, poetry generation,
  query expansion,  transforming declarative sentences into questions, deriving hypotheses for textual entailment, etc. 
Each individual text-to-text application requires that the SMT framework be adapted in several ways:
\begin{itemize}
\item A mechanism for extracting synchronous grammar rules (in this paper we argue that pivot-based paraphrasing is widely applicable)
\item An appropriate set of rule-level features that capture pertinent information about the task
 (i.e.\ whether a rule simplifies a phrase)
\item An appropriate `objective function' that scores the output of the model.  This is a task-specific equivalent to the Bleu metric in SMT.
\item A development set with examples of the sentential transformations that we are modeling.  
%The dev set is used with the objective function to set the weights of the features.
\item Optionally, a way of injecting task-specific rules that were not extracted automatically.
\end{itemize} 
In the remainder of this section, we illustrate how our bilingually extracted paraphrases can be adapted to perform sentence compression,
which is the task of reducing the length of sentence while preserving its core
meaning.  Most previous approaches to sentence compression focused only on the deletion of a subset of words from the sentence \cite{KnightMarcuAI02}.  Our approach follows 
\newcite{cohn-lapata:2008}, who expand the task to include substitutions, insertions and reorderings that are automatically learned from parallel texts.

\subsection{Feature Design}
In Section~\ref{acquisition} we discussed the phrasal and lexical
paraphrasing features $\varphi_{\mathit{lex}}$ and
$\varphi_{\mathit{phrase}}$. While these features quantify the quality
of a paraphrase, they do not make any statement on the change in
language complexity or text length, nor do they bear any information
on the degree and nature of the transformation that the rule
effects. To make this infromation available to the decoder, we enhance
our paraphrases with the following features:
\begin{itemize}
\item A boolean indicator for whether the rule is an identity
  paraphrase, $\delta_{\mathit{identity}}$.

\item Count features $c_{\mathit{src}}$ and $c_{\mathit{tgt}}$
  indicating the number of words on either side of the rule as well as
  two difference features, $c_{\mathit{dcount}} = c_{\mathit{tgt}} -
  c_{\mathit{src}}$ and the analoguosly computed difference in the
  average word length in characters, $c_{\mathit{davg}}$.

\item An incidator for when a rule only contains terminal symbols
  ($\delta_{\mathit{lex}}$) and an indicator for when the source side
  contains terminals, but the target side does not
  ($\delta_{\mathit{del}}$).

\item Indicators for whether the rule swaps the order of two
  nonterminals ($\delta_{\mathit{reorder}}$) and whether the two
  nonterminals are adjacent ($\delta_{\mathit{adj}}$).

\item A rarity penalty $\varphi_{\mathit{rarity}} =
  e^{(1-c_{\mathit{rule}})}$ that quantifies the doubt we may place in
  a rule based on how often we have encountered it in the
  corpus\footnote{Since we do not have an immediate rule count for a
    paraphrase rule $N \rightarrow e_1 | e_2$, we instead estimate
    its rarity penalty as $\varphi_{\mathit{rarity}}(N
    \rightarrow e_1 | e_2) = \max_{f} \min
    \{\varphi_{\mathit{rarity}}(N \rightarrow e_1 | f),
    \varphi_{\mathit{rarity}}(N \rightarrow f | e_2) \}$}.
\end{itemize}


\subsection{Development Data and Objective Function}
To tune the parameters of our paraphrase system to perform sentence
compression, we have created a corpus of compression
paraphrases. Beginning with 9570 tuples of parallel English sentences
obtained from translation references for various machine translation
tasks, we obtained a parallel compression corpus by selecting the
longest reference in each tuple as the source sentence and the
shortest reference as the target sentence and retaining only those
sentence pairs where the word count ratio $r$ was $0.5 < r \leq
0.8$. From these, we then randomly selected 936 sentences for the
development set, as well as 815 sentences for a test set that we later
use to gauge the performance of our system.

Due to the compressive nature of the resulting corpus, we deemed the
widely used BLEU metric \cite{Papineni2002} to be sufficient to judge
the system's compression performance in parameter estimation.



\subsection{Grammar Injections} \label{injection}

As discussed in Section~\ref{analysis}, the paraphrase grammar we
induce is capable of representing a wide variety of
transformations. However, the formalism and extraction method are not
explicitely geared towards a compression application. For instance,
the synchronous nature of our grammar does not allow us to perform
deletions of constituents as done by \newcite{Cohn2007}.  A possible
way to extend the grammar's capabilities in this direction is by
injecting additional rules designed to perform particular
functions. This can include adding rules that allow generic deletions
of target-side nonterminals\footnote{This renders the SCFG
  asynchronous and requires appropriate adjustments in the decoding
  process.}, or generating rules that specifically delete particular
adjectives from the corpus.
\begin{center}
\begin{tabular}{cc}
 $\mathit{JJ} \rightarrow$ & $\mathit{JJ} \mid \varepsilon$ \\
 $\mathit{JJ} \rightarrow$ & superfluous $\mid \varepsilon$ \\
\end{tabular}
\end{center}
In our system, we implemented the injection of rules that improve our
handling of out-of-vocabulary words. Typically an SMT system will
generate heavily penalized identity translation rules for all words in
an input sentence, to assure that the OOV rules will only apply when
there is no translation rule for a particular word.


\subsection{Evaluation} \label{evaluation}

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|}
  \hline
  & Hiero & Syntactic \\
  \hline
  Core features & bad & even better \\
  Full feature set & better & awesome \\
  \hline
\end{tabular}
\end{center}
\caption{TODO: obtain proper scores}
\end{table}


\section{Conclusion} \label{conclusion}

In this work we introduced a method to learn syntactically informed
paraphrases from bilingual parallel texts. We discuss the expressive
power and limitations of our formalism and demonstrate its
applicability to text-to-text generation on the example of sentence
compression.

\bibliographystyle{acl}
\bibliography{paraphrasing}

\nocite{*}

\end{document}

