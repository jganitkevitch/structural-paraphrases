\documentclass[11pt]{article}
\usepackage{acl2010}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
%\setlength\titlebox{6.5cm}    
% You can expand the title box if you really have to

\newcommand{\mnote}[1]{\marginpar{%
  \vskip-\baselineskip
  \raggedright\footnotesize
  \itshape\hrule\smallskip\tiny{#1}\par\smallskip\hrule}}  

\title{Learning Structural and Sentential Paraphrases from Parallel Corpora}

\author{Juri Ganitkevitch \and Chris Callison-Burch\\ 
Center for Language and Speech Processing\\ 
Johns Hopkins University}

\date{}

\begin{document}
\maketitle

\begin{abstract}
Later.
\end{abstract}

\section{Introduction} \label{introduction}

Paraphrases are differing textual entities that convey the same
meaning. The generation and detection of paraphrases is crucial to
many tasks in NLP and has been shown to improve performance in
others. In multi-document summarization systems, paraphrase detection
can be used to recognize and collapse redundancies
\cite{Barzilay1999}. Paraphrasing is also used to improve the coverage
of question answering systems \cite{Ravichandran2002} and for query
expansion in information retrieval systems \cite{Anick1999}. In
machine translation, paraphrases can improve both the coverage of
phrase tables \cite{Callison-Burch2006b} as well as to generate
additional reference translations, leading to better results in
parameter tuning \cite{Madnani2007}. 

The notion of a paraphrase typically denotes a set of surface text
forms with the same meaning:
\begin{center}
\begin{tabular}{c}
the committee's proposal \\
the proposal of the committee
\end{tabular}
\end{center}
These can be generalized into paraphrase patterns such as:
\begin{center}
\begin{tabular}{c}
the $X_1$'s $X_2$ \\
the $X_2$ of the $X_1$
\end{tabular}
\end{center}
Where the $X_i$ are variables or \emph{slots}. 

say that patterns beat phrases, see if literature has examples (Madnani)

In order to achieve more well-formed results, the slots may be
augmented with syntactic constraints, yielding syntactic paraphrases.
\begin{center}
\begin{tabular}{c}
the $NP_1$'s $NP_2$ \\
the $NP_2$ of the $NP_1$
\end{tabular}
\end{center}

claim and underline how constained patterns help 

give examples of their use

point to CCB's paper

say that we're going to incorporate that


need to lead into evaluation a little?

In practice, the nature of paraphrasing as a component of larger
systems makes meaningful standalone evaluation difficult: \mnote{refer
  to some attempts at eval} in many applications of paraphrasing the
proper preservation of meaning and production of grammatical output is
only the basis requirement. A crucially important factor in
determining the system's quality is how well it adheres to the
additional constraints imposed by the task at hand, i.e.\ shortening
the input for sentence compression or generating significantly
differing output for the generation of additional
references. \cite{Madnani2010b}

paraphrasing as rewriting is underspecified for most applications

cite targeted paraphrases as example, 

refer to specific properties on sentence compression as another example

as part of this work, we introduce a means of training a system to
its intended task 

\newpage

\section{Related Work} \label{related_work}

\mnote{what is this about? work that we build on, work that tackles the same
kind of problem}

paraphrase extraction has seen quite a bit of work

key characteristics of an extraction system are the nature of the data and
additional information sources used

to extract paraphrases, you need to find correspondencies

correspondencies can come from two sources (or a combination thereof):
distributional similarity and prior established parallelism
(i.e. parallel corpora)

distributional similarity features have been primarily leveraged on
monolingual data, often coupled with dependency parses or
part-of-speech tagging

advantages of monolingual datasets are primarily in volume

can be leveraged to make up for fuzziness of distributional features




A number of approaches to corpus-based English paraphrase extraction
have been presented in the past. Typically, the methods differ in the
nature of the data and additional annotations or information they
use. Roughly, we can distinguish between three types of corpora:
monolingual, parallel monolingual and parallel bilingual.

Monolingually sourced approaches make use of the abundance of plain
text ressources and the good quality of parsing and tagging machinery
on English. However, lacking any in order to find paraphrases they are
limited to distibutional similarity features.

Some approaches employ monolingual collections of text and rely on
distributional similarity features to extract parahrase patterns,
using dependency parses \cite{Lin2001} or part-of-speech tags
\cite{Bhagat2008} to further inform their algorithms.

Another branch of paraphrase research employs sentence-aligned
monolingual parallel corpora \cite{Barzilay2001,Quirk2004} to

There have been many proposed approaches to paraphrase induction, most
of which can be placed in two groups, according to what data they use
to source their method. Monolingual approaches typically make use of
vast amounts of English text to extract paraphrases. Due to the
availability of good syntactic and dependency parsers, these methods
are often able to infer \emph{structural} paraphrases, i.e.\
paraphrastic patterns that capture either syntactic or semantic
information and can be generalize via ``slots''. However, the coverage
of such systems tends to be low.

Bilingually sourced approaches, on the other hand make use of the
relative abundance of sentence-parallel corpora and extract bilingual
tables of phrases (cite Chris) or patterns (cite Nitin, Zhao) from
which paraphrases can be extracted by pivoting over the non-English
side of the table. However, due to the nature of bilingual phrase
tables, the resulting paraphrases are often restricted to
surface-level.

\emph{not quite true, since Zhao does extract labled pattern based on
  dependency graphs. need to look into that and categorize, organize
  prior work properly.}

Should distinguish between phrasal and structural paraphrases, as well
as bilingually and monolingually sourced approaches to extraction.

Perhaps: translation as bilingual paraphrasing, and how that thought
brings about the pivot approach.

Reference monolingual approaches that are structural, pivot-based
approaches that are phrasal, Nitin's stuff.

Note how our approach sort-of unifies the two; there's an analogy to
Chris' syntactic constraints as well as the obvious step from Hiero to
SAMT.

\newpage

\section{Paraphrase Acquisition} \label{acquisition}

The work presented in this paper falls into the category of
bilingually-sourced pivot-based approaches. We make use of established
machine translation machinery to extract syntactically informed
paraphrase patterns from multiple bilingual parallel corpora.


\subsection{The SAMT Formalism}

The \emph{Syntax Augmented Machine Translation} (SAMT,
\cite{Zollmann2006})

Formal: Synchronous grammars, with the usual examples (one phrasal, one
structural).

Formal: Log-linear model for features, weights will be optimized to some
objective, we discuss those in later section.

Diagram for grammar extraction (two sentence pairs with trees and
alignments, show how that gets us a paraphrase pattern). 



The method we present in this work extends Nitin's pivot-based Hiero
paraphrasing approach to the richer, syntax-informed SAMT
formalism. Starting with a bilingual parallel corpus, we use the
familiar MT and parsing (cite!) machinery to word-align the data and
extract an SAMT-style grammar.

Elaborate on this, reference the appropriate work. This is more about
the pivoting than it is about the MT-style application of the
paraphrases.

In \ldots we give a brief description of the
data sets used in our experiments and outline the tools used to
process them. Section~\ref{extraction} elaborates on the extraction
of the bilingual translation grammars and their transformation into
monolingual paraphrasing grammars.

\subsection{Paraphrase Grammar Extraction} \label{extraction}

Some talk about the SAMT approach needs to go here. 

We extract SAMT translation grammars for nine languages. Give some
details on the pipeline, mention that the grammars are
\emph{gargantuan} (this word needs to be in the paper!), but that the
whole process is very well-suited for MapReduce (even though, we
didn't use it).

\subsection{Creating Paraphrase Rules} \label{rule_creation}

\subsubsection{Rule Body} \label{rule_body}

To create paraphrase rules from bilingual translation rules, we pivot
over the foreign side of the translation rule, with the additional
constraint that the rule's head, i.e.\ the label that governs the
rule.

Mention the proper mapping and flipping of nonterminals.

\subsubsection{Rule Features} \label{rule_features}

The SAMT grammars our paraphrasing system is based on provide a rich
feature set that takes into account source- and target-side
frequencies, reordering of NTs and lexical translation probablities
for each rule. When transforming the bilingual grammars into a
monolingual paraphrase grammar we preseve the feature set and

Shouldn't give details on every single feature, but point out some key
approaches:
\begin{itemize}
\item probablistic features multiply 
\item target-side indicator features are inherited
\item actually state what happens with other indicator features
  (re-ordering, punctuation, rareness etc)
\end{itemize}

\newpage

\section{Task-Specific Parameter Estimation} 
\label{adaptation}

have to estimate parameters now

general approach: decide on an objective function, choose parameters
such that

Having extracted a paraphrase grammar, we are left with the task of
estimating the parameters $\lambda_i$ of the log-linear
model. Generally, the $\lambda_i$ are chosen to maximize a given
objective function. Ideally, the objective function mimics the
system's intended use, so that parameter estimation tunes the system
to optimal practical performance.

Many previously proposed paraphrase extraction methods focus on
maximizing recall and precision over a manually annotated set of
paraphrase patterns \cite{Zhao2008,Bhagat2008}.  Similarly, paraphrase
approaches that utilize machine translation machinery do not stray
from the usual pipeline, using MERT \cite{Och2003} to tune the
paraphrase system to produce good English output by maximizing BLEU
score on a set of known paraphrases \cite{Madnani2007}.

While these methods may produce well-weighted collections of
paraphrase patterns, they do not emphasize 

it results in a paraphrase system that is not
focussed on any particular task.



\subsection{Reference Expansion}

We present an examplary application of our paraphrasing system

Using paraphrases for reference expansion, cite Madnani

idea behind reference expansion: create additional ``reference''
translations to tune a machine translation system

however, tuning our paraphrase system to BLEU results in a paraphraser
that generates only minimal changes \mnote{if so, how did Madnani deal
with that?}

We therefore implement a paraphrase-specific metric, ppBLEU, that aims
to amend this issue.

\subsubsection{Paraphrase BLEU} \label{pp_bleu}

the familiar BLEU metric (cite BLEU) quantifies the quality of a
translation by measuring the n-gram overlap between the generated
translation and a set of  reference translation

for paraphrasing, this means that we tune the system to generate


\subsubsection{Data} \label{data}

We use Europarl v. 5. Align with Berkeley and parse with The Parser.

\subsubsection{Evaluation} \label{evaluation}

A straightforward way to evaluate a paraphrasing system is by using to
improve an SMT system's performance. Cite Chris and Nitin. Compare to
Hiero baseline.

\subsection{Possibly Another Paraphrase
  Application} \label{other_application}

Maybe summarization or sentence compression? Is there
any ``hard'' eval to be had? Human via MTurk?

\newpage

\section{Analysis} \label{analysis}

one of the motivations for our approach was the expectation of being
able to acquire structural paraphrases, especially paraphrases taht
are capable of learning long-distance transformations such as
passivization, dative shift, possessive something and prepositional
paraphrases.

\subsection{Sentential Paraphrasing} \label{sentential_paraphrasing}

We are interested in sentential paraphrases. Why are we interested?
More powerful than locally constrained, gives us large-scale changes
to sentential structure, which can be cruicial to applications such as
detecting entailment or automatically creating significantly differing
references. \emph{However, phrasal decoder-based approaches should be
  able to achieve similar re-ordering effects (if not the generality,
  which we only implicitely achieve, really). Maybe we should add a
  phrase-based baseline in addition to Hiero? Did Nitin talk about
  this?}

While the definition of a phrasal paraphrase is intuitively clear,
sentential paraphrases are much harder to define. When paraphrasing a
sentence $s$ into a new sentence $t$, the term suggests that we expect
the changes to $s$ to be above a certain threshold for $t$ to be
considered a sentential paraphrase. 

\newpage

\section{Conclusion} \label{conclusion}

Look, we unified everything in the field and made all this stuff from
the previous section much better. Or did we?

\newpage

\bibliographystyle{acl}
\bibliography{paraphrasing}

\nocite{*}

\end{document}

